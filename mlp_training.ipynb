{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2254,"status":"ok","timestamp":1644236470537,"user":{"displayName":"巨峰","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ312ml7sKAt5HJP_fBP-ooQm_1_LlrntDA9sW0g=s64","userId":"02610628405275685183"},"user_tz":-540},"id":"oDJWpyf2wRin","outputId":"d9f73bd4-6067-4e05-c853-100602e561e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive/Kaggle/UbiquantMarketPredictionDrive/zume/ubiquant_src\"\n","!git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":470,"status":"ok","timestamp":1644236471668,"user":{"displayName":"巨峰","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ312ml7sKAt5HJP_fBP-ooQm_1_LlrntDA9sW0g=s64","userId":"02610628405275685183"},"user_tz":-540},"id":"iO6Pl5tAvcxN","outputId":"a0d6ca9e-6a3b-40b2-84cc-6839220e1a32"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: nvidia-smi: command not found\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MrvRuLrvcxP"},"outputs":[],"source":["class Config:\n","    author = \"zume\" # Your name\n","    competition = \"ubiquant-market-prediction\"\n","    name = \"MLP\" # The name of the Dataset\n","    upload_from_colab = True # If True, the model uploads to the Kaggle Dataset\n","    \n","    colab_dir = \"/content/drive/MyDrive/Kaggle/UbiquantMarketPredictionDrive\" # Your own directory\n","    drive_path = colab_dir + f\"/{author}\"\n","    api_path = \"/content/drive/MyDrive/Kaggle/kaggle.json\" # Your own api-path\n","    \n","    dataset_path = ['robikscube/ubiquant-parquet'] # The dataset you want to download\n","\n","    n_fold = 5\n","    n_test = 2\n","    purge = 10\n","    embargo = 0.01\n","    \n","    from scipy.special import comb\n","    trn_fold = [i for i in range(comb(n_fold, n_test, exact=True))]\n","    seed = 42\n","    max_epochs = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4BWPj1qvcxR"},"outputs":[],"source":["import os\n","import gc\n","import sys\n","import json\n","import pickle\n","import shutil\n","import random\n","import joblib\n","import requests\n","import itertools\n","\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from glob import glob\n","from tqdm.auto import tqdm\n","\n","from logging import StreamHandler, FileHandler, Formatter, getLogger, DEBUG, INFO\n","\n","from sklearn.preprocessing import (\n","    StandardScaler,\n","    MinMaxScaler,\n","    RobustScaler,\n",")\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import tensorflow as tf\n","\n","sys.path.append(\"/content/drive/MyDrive/Kaggle/UbiquantMarketPredictionDrive/zume/ubiquant_src\")\n","import mycv\n","\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","def MAE(y_true, y_pred):\n","    return mean_absolute_error(y_true, y_pred)\n","\n","def MSE(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred)\n","\n","def RMSE(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred, squared=False)\n","\n","def PearsonR(y_true, y_pred):\n","    return scipy.stats.pearsonr(y_true, y_pred)[0]\n","\n","def PearsonR_metric(y_true, y_pred):\n","    return \"pearsonr\", scipy.stats.pearsonr(y_true, y_pred)[0], True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class UbiquantDataset(Dataset):\n","    def __init__(self, df, mode=\"train\"):\n","        self.mode = mode\n","        self.features = df[[f\"f_{i}\" for i in range(300)]].values\n","        if self.mode != \"test\":\n","            self.targets = df[\"target\"].values\n","        self.len = df.shape[0]\n","\n","    def __len__(self):\n","        return self.len\n","    \n","    def __getitem__(self, index):\n","        if self.mode != \"test\":\n","            return self.features[index], self.targets[index]\n","        else:\n","            return self.features[index]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def swish(x):\n","    return x * torch.sigmoid(x)\n","\n","class MLP(nn.Module):\n","    def __init__(self, n_features):\n","        super().__init__()\n","\n","        self.l1 = nn.Linear(n_features, 64)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.l2 = nn.Linear(64, 64)\n","        self.bn2 = nn.BatchNorm1d(64)\n","        self.l3 = nn.Linear(64, 32)\n","        self.bn3 = nn.BatchNorm1d(32)\n","        self.l4 = nn.Linear(32, 1)\n","    \n","    def forward(self,x):\n","        x = swish(self.l1(x))\n","        x = self.bn1(x)\n","        x = swish(self.l2(x))\n","        x = self.bn2(x)\n","        x = swish(self.l3(x))\n","        x = self.bn3(x)\n","        x = swish(self.l4(x))\n","        return x\n","    \n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_fn(cfg, train, fold, folds):\n","    model = MLP(300).to(device)\n","    loss_fn = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1, mode=\"min\")\n","    epochs = 8\n","\n","    idx_train = ((folds==fold).groupby(level=0).sum()==0)\n","    idx_valid = ((folds==fold).groupby(level=0).sum()>=1)\n","\n","    train_set = UbiquantDataset(train[idx_train], mode=\"train\")\n","    valid_set = UbiquantDataset(train[idx_valid], mode=\"valid\")\n","    dataloaders = {\n","        \"train\": DataLoader(train_set, batch_size=512, num_workers=4, pin_memory=True),\n","        \"valid\": DataLoader(valid_set, batch_size=512, num_workers=4, pin_memory=True)\n","        }\n","        \n","    num_train = len(dataloaders[\"train\"])\n","    num_valid = len(dataloaders[\"valid\"])\n","\n","    losses = []\n","    best_loss = np.inf\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        for i, (features, targets) in enumerate(dataloaders[\"train\"]):\n","            features = features.to(device)\n","            targets = targets.unsqueeze(1).to(device)\n","\n","            y = model(features)\n","            loss = loss_fn(y, targets)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_epoch_loss = train_loss / num_train\n","\n","        model.eval()\n","        valid_preds = []\n","        valid_loss = 0\n","        with torch.no_grad():\n","            for i, (features, targets) in enumerate(dataloaders[\"valid\"]):\n","                features = features.to(device)\n","                targets = targets.unsqueeze(1).to(device)\n","\n","                y = model(features)\n","                loss = loss_fn(y, targets)\n","                \n","                valid_loss += loss.item()\n","                valid_preds.extend(y.detach().cpu().numpy().flatten())\n","            valid_epoch_loss = valid_loss / num_valid\n","        \n","        scheduler.step(valid_epoch_loss)\n","        oof = train[idx_valid][['target']].copy()\n","        oof['pred'] = valid_preds\n","        score = oof['pred'].corr(oof['target'])\n","\n","        losses.append((train_epoch_loss, valid_epoch_loss))\n","        logger.info(f\"EPOCH:{epoch}, LR:{optimizer.param_groups[0]['lr']}\")\n","        logger.info(f\"Train loss: {train_epoch_loss:.6f}\")\n","        logger.info(f\"Valid loss: {valid_epoch_loss:.6f}\")\n","        logger.info(f\"PearsonR score: {score:.6f}\")\n","\n","        if best_loss > valid_epoch_loss:\n","            torch.save(model.state_dict(), os.path.join(cfg.EXP_MODEL, f\"model_{fold}.pth\"))\n","            best_loss = valid_epoch_loss\n","    \n","    return losses, oof"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKeJYuRvvcxU"},"outputs":[],"source":["def setup(cfg):\n","    cfg.COLAB = 'google.colab' in sys.modules\n","    if cfg.COLAB:\n","        global logger\n","        print('This environment is Google Colab')\n","        \n","        # mount\n","        from google.colab import drive\n","        if not os.path.isdir('/content/drive'):\n","            drive.mount('/content/drive') \n","        \n","        # import library\n","        ! pip install --quiet tensorflow-addons\n","\n","        # use kaggle api (need kaggle token)\n","        f = open(cfg.api_path, 'r')\n","        json_data = json.load(f) \n","        os.environ['KAGGLE_USERNAME'] = json_data['username']\n","        os.environ['KAGGLE_KEY'] = json_data['key']\n","\n","        # set dirs\n","        cfg.DRIVE = cfg.drive_path\n","        cfg.EXP = (cfg.name if cfg.name is not None \n","            else requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name'][:-6]\n","        )\n","        cfg.INPUT = os.path.join(cfg.DRIVE, 'Input')\n","        cfg.OUTPUT = os.path.join(cfg.DRIVE, 'Output')\n","        cfg.SUBMISSION = os.path.join(cfg.DRIVE, 'Submission')\n","        cfg.DATASET = os.path.join(cfg.DRIVE, 'Dataset')\n","\n","        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n","        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","        \n","        if not os.path.isfile(os.path.join(cfg.INPUT, 'train.csv')):\n","            # load dataset\n","            ! pip install --upgrade --force-reinstall --no-deps kaggle\n","            ! kaggle competitions download -c $cfg.competition -p $cfg.INPUT\n","            filepath = os.path.join(cfg.INPUT,cfg.competition+'.zip')\n","            ! unzip -d $cfg.INPUT $filepath\n","            \n","        \n","        for path in cfg.dataset_path:\n","            datasetpath = os.path.join(cfg.DATASET,  path.split('/')[1])\n","            if not os.path.exists(datasetpath):\n","                os.makedirs(datasetpath, exist_ok=True)\n","                ! kaggle datasets download $path -p $datasetpath\n","                filepath = os.path.join(datasetpath, path.split(\"/\")[1]+'.zip')\n","                ! unzip -d $datasetpath $filepath\n","\n","        logger = getLogger(\"main\")\n","        logger.setLevel(DEBUG)\n","        formatter = Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","        h = FileHandler(os.path.join(cfg.OUTPUT_EXP ,\"train.log\"))\n","        h.setLevel(INFO)\n","        h.setFormatter(formatter)\n","        sh = StreamHandler()\n","        sh.setLevel(DEBUG)\n","        sh.setFormatter(formatter)\n","        logger.addHandler(h)\n","        logger.addHandler(sh)\n","    \n","    else:\n","        print('This environment is Kaggle Kernel')\n","\n","        # set dirs\n","        cfg.INPUT = f'../input/{cfg.competition}'\n","        cfg.EXP = cfg.name\n","        cfg.OUTPUT_EXP = cfg.name\n","        cfg.SUBMISSION = './'\n","        cfg.DATASET = '../input/'\n","        \n","        cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","\n","    seed_everything(cfg.seed)\n","    return cfg\n","\n","\n","def dataset_create_new(dataset_name, upload_dir):\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = dataset_name\n","    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","    api = KaggleApi()\n","    api.authenticate()\n","    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fit_mlp(cfg, train, folds):\n","    oof_list = []\n","\n","    for fold in cfg.trn_fold:\n","        losses, oof = train_fn(cfg, train, fold, folds)\n","        oof_list.append(oof)\n","    oof = pd.concat(oof_list)\n","    logger.info(f\"OOF PearsonR score: {oof['pred'].corr(oof['target'])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4858,"status":"ok","timestamp":1644236476944,"user":{"displayName":"巨峰","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ312ml7sKAt5HJP_fBP-ooQm_1_LlrntDA9sW0g=s64","userId":"02610628405275685183"},"user_tz":-540},"id":"vASSOwJQvcxX","outputId":"714e8923-a069-445c-9e1d-0f2eb07291ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["This environment is Google Colab\n"]}],"source":["# =========================\n","# SetUp\n","# =========================\n","Config = setup(Config)\n","logger.info(\"Parameters\")\n","logger.info(\"seed: \" + f\"{Config.seed}\")\n","logger.info(\"n_fold: \" + f\"{Config.n_fold}\")\n","logger.info(\"n_test: \" + f\"{Config.n_test}\")\n","logger.info(\"purge: \" + f\"{Config.purge}\")\n","logger.info(\"embargo: \" + f\"{Config.embargo}\")\n","\n","# 2nd import\n","import tensorflow_addons as tfa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57791,"status":"ok","timestamp":1644236534727,"user":{"displayName":"巨峰","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ312ml7sKAt5HJP_fBP-ooQm_1_LlrntDA9sW0g=s64","userId":"02610628405275685183"},"user_tz":-540},"id":"ftCfHhdWvcxX","outputId":"97d56b56-eea7-438f-cbe7-f67e2240dda4"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# =========================\n","# Pre-Processing\n","# =========================\n","train = pd.read_parquet(os.path.join(Config.DATASET, 'ubiquant-parquet/train_low_mem.parquet'))\n","folds = mycv.get_CPGKfold(train, 'target', 'time_id', Config.n_fold, Config.n_test, Config.purge, Config.embargo)\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"elapsed":53984,"status":"error","timestamp":1644236588694,"user":{"displayName":"巨峰","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJ312ml7sKAt5HJP_fBP-ooQm_1_LlrntDA9sW0g=s64","userId":"02610628405275685183"},"user_tz":-540},"id":"DfOhSDdrvcxY","outputId":"330ad5db-4fe7-4c70-bf8e-0e2fe71a8813"},"outputs":[],"source":["# =========================\n","# Training & Upload\n","# =========================\n","\n","fit_mlp(cfg=Config, train=train, folds=folds)\n","\n","# upload output folder to kaggle dataset\n","if Config.upload_from_colab:\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","    dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wbym_w6gvcxZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","name":"ubiquant-training-google-colaboratory-training.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}
